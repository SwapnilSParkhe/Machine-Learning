{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection using Anomaly Detection | Part 2b (Tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: Predict whether a credit card transaction is fraudulent or not based on its details. Extract the patterns that hint towards fraud by modeling the past transactions such that all frauds are detected and false positives are minimised.\n",
    "\n",
    "**Evaluation**: Recall, PR-AUC, f1, Precision @t will be used for fine-tuning and evaluation using available labels\n",
    "\n",
    "**Potential Solution Framework**: Since we have enough labeled data, we are using the fully-supervised anomaly detection setting (learning data structure from labels) using below two approaches. Note: One this common though, we would be trying to learn the underlying \"normal\" distribution & draw threshold boundary to weed out anomalies\n",
    "\n",
    "**-----b. \"End-to-end fully-supervised\"-----**\n",
    "    - pass whatever (normal or anomolous or both) that available training-fold labeled data is\n",
    "    - then use those models to check and validate on whatever that remaining available valid-fold data is\n",
    "    - Pros/Cons:\n",
    "      i. We could use gridsearchcv directly\n",
    "      ii. OCSVM would not perform well on such mixed data**\n",
    "\n",
    "**Existing intuitions on algorithms (based on performance on 2D datasets)** -\n",
    "Source (https://scikit-learn.org/stable/auto_examples/plot_anomaly_comparison.html#sphx-glr-auto-examples-plot-anomaly-comparison-py):\n",
    "- IF and LOF are good when we have multimodal data. LOF is better when modes have different desities (local aspect of LOF)\n",
    "- OCSVM is sensitive to outliers and doesn't generally perform well for OD (but good for ND when training data is uncontaminated), but depending on values of hyperparamters it could still give useful results\n",
    "- EllipticEnvelope assumes Gaussian distribution and thus learns ellipse. Not good for multimodal data but robust to outliers\n",
    "\n",
    "**Useful material and references**\n",
    "- https://escholarship.org/uc/item/1f03f6hb#main\n",
    "- https://www.hindawi.com/journals/complexity/2019/2686378/\n",
    "- https://imada.sdu.dk/~zimek/InvitedTalks/TUVienna-2016-05-18-outlier-evaluation.pdf\n",
    "- https://www.gta.ufrj.br/~alvarenga/files/CPE826/Ahmed2016-Survey.pdf\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd; pd.set_option('display.max_columns', None)\n",
    "import matplotlib.pyplot as plt; \n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "\n",
    "#Importing data processing and prep libraries\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler   #RobustScaler robust to outliers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV  #for hyperparameter tuning\n",
    "\n",
    "#Importing machine learning algo libraries\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "#Importing evaluation focussed libraries\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import f1_score, recall_score, average_precision_score\n",
    "\n",
    "#Other useful libraries\n",
    "#!pip install missingno   \n",
    "import missingno as missviz   #Custom library for missing value inspections\n",
    "from sklearn.manifold import TSNE   #For visualising high dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details of training data - shape of predictor matrix is (227845, 30) and # ones in target series is 389\n",
      "Details of testing data - shape of predictor matrix is (56962, 30) and # ones in target series is 103\n"
     ]
    }
   ],
   "source": [
    "#Importing\n",
    "with open(\"ADS_b.pkl\", \"rb\") as f:\n",
    "    X_train, X_test, y_train, y_test, X_cols, y_cols = pickle.load(f)\n",
    "    \n",
    "#Basic details\n",
    "print(\"Details of training data - shape of predictor matrix is {I} and # ones in target series is {J}\".format(I=X_train.shape, J=sum(y_train)))\n",
    "print(\"Details of testing data - shape of predictor matrix is {I} and # ones in target series is {J}\".format(I=X_test.shape, J=sum(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter tuning and model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Splitting data into \"small\" and \"big\" parts**\n",
    "- Note: We'll train and tune using the whole data in Google colab with GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting train and test into \"small\" and \"big\" parts to expedite training and tuning\n",
    "seed=123\n",
    "small_frac=0.05\n",
    "X_train_big, X_train_small, y_train_big, y_train_small = train_test_split(X_train, y_train, test_size=small_frac, stratify=np.array(y_train), random_state=seed)\n",
    "X_test_big, X_test_small, y_test_big, y_test_small = train_test_split(X_test, y_test, test_size=small_frac, stratify=np.array(y_test), random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Creating dictionary of \"intialized\" models and their hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "models={\n",
    "    \"IF\": IsolationForest(random_state=seed),\n",
    "    \"LOF\": LocalOutlierFactor(novelty=True),\n",
    "    \"OCSVM\": OneClassSVM(random_state=seed)}\n",
    "\n",
    "model_hp={\n",
    "    \"IF\": {\"contamination\": [0.0001, 0.001, 0.0025, 0.005, 0.01], \"max_samples\": list(range(10,300,60)), \"n_estimators\": [10,50,100,200,500]},\n",
    "    \"LOF\": {\"contamination\": [0.0001, 0.001, 0.0025, 0.005, 0.01], \"n_neighbors\": [5,10,20,50,100]},\n",
    "    \"OCSVM\": {\"nu\": [0.0001, 0.001, 0.0025, 0.005, 0.01], \"kernel\": [\"linear\", \"rbf\", \"poly\"], \"gamma\": np.power(10.0, range(-3,2))}}\n",
    "\n",
    "def my_f1_score(model, X, y):\n",
    "    y_pred=model.predict(X)\n",
    "    return f1_score(y, np.where(y_pred==1,0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 Training and tuning models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Running GridsearchCV for IF-----\n",
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:   27.2s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:   31.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   37.3s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   44.0s\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:   50.2s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   59.2s\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 322 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.03 s, sys: 1.6 s, total: 5.62 s\n",
      "Wall time: 2min 38s\n",
      "-----Running GridsearchCV for LOF-----\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   18.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   36.5s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   47.1s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  68 out of  75 | elapsed:  2.5min remaining:   15.5s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.38 s, sys: 255 ms, total: 7.63 s\n",
      "Wall time: 2min 45s\n",
      "-----Running GridsearchCV for OCSVM-----\n",
      "Fitting 3 folds for each of 75 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:   34.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   40.1s\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed:  1.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.15 s, sys: 302 ms, total: 2.45 s\n",
      "Wall time: 1min 16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:  1.3min finished\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "\n",
    "for model_name in models.keys():\n",
    "    print(\"-----Running GridsearchCV for {M}-----\".format(M=model_name))\n",
    "    model=models[model_name]\n",
    "    hp=model_hp[model_name]\n",
    "    GSCV=GridSearchCV(model, hp, cv=3, scoring=my_f1_score, n_jobs=-1, verbose=10)\n",
    "    %time GSCV.fit(X_train_small, y_train_small)\n",
    "    results.append([model_name, GSCV.best_params_, GSCV.best_score_, pd.DataFrame(GSCV.cv_results_)])\n",
    "    \n",
    "results_df=pd.DataFrame(results, columns=[\"model_name\", \"best_hp\", \"best_score\", \"CV_results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.396827\n",
       "1    0.599412\n",
       "2    0.245169\n",
       "Name: CV_results, dtype: float64"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[\"CV_results\"].apply(lambda x: x[\"mean_test_score\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
