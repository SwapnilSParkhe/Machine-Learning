{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection using Anomaly Detection | Part 2 (Tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: Predict whether a credit card transaction is fraudulent or not based on its details. Extract the patterns that hint towards fraud by modeling the past transactions such that all frauds are detected and false positives are minimised.\n",
    "\n",
    "**Solution Framework**: Using \"novelty detection\" models in a semi-supervised setting which are used to:\n",
    "- learn what could be the underlying \"normal\" distribution, by training models on a set of available labeled \"normal\" data points\n",
    "- then use those models to validate on \"perceived novel\" (both normal and anomolous) data points to check what and how much gets predicted as normal or anomalous. \n",
    "- Recall, PR-ROC, Precision @t will be used for fine-tuning and evaluation using available labels\n",
    "\n",
    "**Existing intuitions on algorithms (based on performance on 2D datasets)** -\n",
    "Source (https://scikit-learn.org/stable/auto_examples/plot_anomaly_comparison.html#sphx-glr-auto-examples-plot-anomaly-comparison-py):\n",
    "- IF and LOF are good when we have multimodal data. LOF is better when modes have different desities (local aspect of LOF)\n",
    "- OCSVM is sensitive to outliers and doesn't generally perform well for OD (but good for ND when training data is uncontaminated), but depending on values of hyperparamters it could still give useful results\n",
    "- EllipticEnvelope assumes Gaussian distribution and thus learns ellipse. Not good for multimodal data but robust to outliers\n",
    "\n",
    "**Useful material and references**\n",
    "- https://escholarship.org/uc/item/1f03f6hb#main\n",
    "- https://www.hindawi.com/journals/complexity/2019/2686378/\n",
    "- https://imada.sdu.dk/~zimek/InvitedTalks/TUVienna-2016-05-18-outlier-evaluation.pdf\n",
    "- https://www.gta.ufrj.br/~alvarenga/files/CPE826/Ahmed2016-Survey.pdf\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "\n",
    "#Importing data processing and prep libraries\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler   #RobustScaler robust to outliers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold  #for hyperparameter tuning\n",
    "\n",
    "#Importing machine learning algo libraries\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "#Importing evaluation focussed libraries\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import f1_score, recall_score, average_precision_score\n",
    "\n",
    "#Other useful libraries\n",
    "#!pip install missingno   \n",
    "import missingno as missviz   #Custom library for missing value inspections\n",
    "from sklearn.manifold import TSNE   #For visualising high dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting Data\n",
    "Note: We had saved data as **ADS.pkl** in our working directory from part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the required ADS arrays and their column names\n",
    "with open(\"ADS.pkl\", \"rb\") as f:\n",
    "    X_train, y_train, X_test, y_test, X_cols, y_cols = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tuning and Model Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling the data first to expedite model training (we'll use \"small\" data for now, and full data later based on understanding developed from the results on sample)\n",
    "seed=123\n",
    "small_frac=0.05\n",
    "X_train_big, X_train_small, y_train_big, y_train_small = train_test_split(X_train, y_train, test_size=small_frac, random_state=seed)\n",
    "X_test_big, X_test_small, y_test_big, y_test_small = train_test_split(X_test, y_test, test_size=small_frac, stratify=y_test, random_state=seed)\n",
    "\n",
    "#Separting \"perceived novel\" test data into validation (for hyperparameter tuning) and test_ (for performance evaluation)\n",
    "X_valid_small_, X_test_small_, y_valid_small_, y_test_small_ = train_test_split(X_test_small, y_test_small, test_size=0.2, stratify=y_test_small, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list of models or anomaly detectors\n",
    "modelname_small=[\"IF\", \"LOF\", \"OCSVM\"]\n",
    "\n",
    "#Creating a dictionary of classifiers and their corresponding hyperparameters grid\n",
    "hyperparams_small={\n",
    "    \"IF\": {\"contamination\": [0.001, 0.01, 0.025, 0.05, 0.1], \"max_samples\": list(range(10,300,60)),\"n_estimators\": [10, 50, 100, 200, 500]},\n",
    "    \"LOF\": {\"contamination\": [0.001, 0.01, 0.025, 0.05, 0.1],\"n_neighbors\": [5, 10, 20, 50, 100]},\n",
    "    \"OCSVM\": {\"nu\": [0.001, 0.01, 0.025, 0.05, 0.1], \"kernel\": ['poly', 'rbf', 'linear'], \"gamma\": np.power(10.0, np.arange(-3,2))}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training models\n",
    "results=[]\n",
    "\n",
    "\"\"\"Hyperparameter tuning using K-fold crossvalidation:\n",
    "    1. K-Split inliers data into train-1 and valid-1 \n",
    "    2. K-Split inlier + outlier data (\"new\") into train-2 and valid-2\n",
    "    3. Training on each train-1, and validating with each train-2 (K*K outputs)\n",
    "\n",
    "   Note: This was we will:\n",
    "    1. train model on multiple and varying sets of inliers \n",
    "    2. validate on multiple and varying sets of inliers and outliers\n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "#Splitting training (inliers) data into train-1 and valid-1\n",
    "kf_in=KFold(n_splits=5, random_state=seed)\n",
    "kf_in_out=KFold(n_splits=5, random_state=seed)\n",
    "for i, (train_small_index, valid_small_index) in enumerate(kf_in.split(X_train_small)):\n",
    "    X_train_small_1, X_valid_small_1 = X_train_small[train_small_index], X_train_small[valid_small_index]\n",
    "    y_train_small_1, y_valid_small_1 = y_train_small[train_small_index], y_train_small[valid_small_index]\n",
    "    print(\"------Passsing training fold # {I} which has {L} obs------\".format(I=i+1, L=len(X_train_small_1)))\n",
    "\n",
    "    #Splitting validation (perceivd novel) data into train-2 and valid-2\n",
    "    for j, (train_small_index_, valid_small_index_) in enumerate(kf_in_out.split(X_valid_small_)):\n",
    "        X_train_small_2, X_valid_small_2 = X_valid_small_[train_small_index_], X_valid_small_[valid_small_index_]\n",
    "        y_train_small_2, y_valid_small_2 = y_valid_small_[train_small_index_], y_valid_small_[valid_small_index_]\n",
    "        print(\"------Passsing validation fold # {J} which has {M} outliers------\".format(J=j+1, M=sum(y_train_small_2)))\n",
    "        \n",
    "        #Iterating through mutiple models and their hyperparameters\n",
    "        for modelname in modelname_small:\n",
    "            print(\"------{}(and available hyperparams)--------\\n\".format(modelname))\n",
    "            print(hyperparams_small[modelname], \"\\n\")\n",
    "\n",
    "            fig, axs = plt.subplots(1,3, figsize=(16,5), sharey=True)\n",
    "            axs=axs.flatten()\n",
    "            if modelname==\"IF\":\n",
    "                for p in hyperparams_small[modelname][\"contamination\"]:\n",
    "                    for q in hyperparams_small[modelname][\"max_samples\"]:\n",
    "                        for r in hyperparams_small[modelname][\"n_estimators\"]:\n",
    "\n",
    "                            #Instantiating model\n",
    "                            model=IsolationForest(contamination=p, max_samples= q, n_estimators=r, random_state=seed)\n",
    "                            model.fit(X_train_small_1)\n",
    "                            y_scores=model.decision_function(X_train_small_2)\n",
    "                            y_pred=model.predict(X_train_small_2)\n",
    "\n",
    "                            #The algos predicts {1,-1} for {normal, anomalous} case, whereas original y has {0,1}\n",
    "                            y_pred=list(map(lambda x: 1 if(x==-1) else 0, y_pred))\n",
    "\n",
    "                            #Collating results\n",
    "                            results.append([i+1, j+1, modelname, p, q, r, recall_score(y_train_small_2,y_pred), f1_score(y_train_small_2,y_pred), average_precision_score(y_train_small_2,y_scores)])\n",
    "\n",
    "                #Dumping results into a df for pandas manipulation\n",
    "                results_df=pd.DataFrame(results, columns=[\"Train\", \"Valid\", \"model\", \"p\", \"q\", \"r\", \"recall\", \"f1\", \"PR-AUC\"])\n",
    "\n",
    "                #Best hyperparam combinations of model for iterating train-valid \n",
    "                print(results_df.groupby([\"model\", \"p\", \"q\", \"r\"])[[\"f1\"]].mean().idxmax())\n",
    "                \n",
    "                #Plotting results (hyperparams vs mean f1)\n",
    "                for i,v in enumerate([\"p\", \"q\", \"r\"]):\n",
    "                    results_df.groupby([v])[\"f1\"].mean().plot(ax=axs[i])\n",
    "                plt.show()\n",
    "                \n",
    "            elif modelname==\"OCSVM\":\n",
    "                for p in hyperparams_small[modelname][\"nu\"]:\n",
    "                    for q in hyperparams_small[modelname][\"kernel\"]:\n",
    "                        for r in hyperparams_small[modelname][\"gamma\"]:\n",
    "\n",
    "                            #Instantiating model\n",
    "                            model=OneClassSVM(nu=p, kernel=q, gamma=r, random_state=seed)\n",
    "                            model.fit(X_train_small_1)\n",
    "                            y_scores=model.decision_function(X_train_small_2)\n",
    "                            y_pred=model.predict(X_train_small_2)\n",
    "\n",
    "                            #The algos predicts {1,-1} for {normal, anomalous} case, whereas original y has {0,1}\n",
    "                            y_pred=list(map(lambda x: 1 if(x==-1) else 0, y_pred))\n",
    "\n",
    "                            #Collating results\n",
    "                            results.append([i+1, j+1, modelname, p, q, r, recall_score(y_train_small_2,y_pred), f1_score(y_train_small_2,y_pred), average_precision_score(y_train_small_2,y_scores)])\n",
    "\n",
    "                #Dumping results into a df for pandas manipulation\n",
    "                results_df=pd.DataFrame(results, columns=[\"Train\", \"Valid\", \"model\", \"p\", \"q\", \"r\", \"recall\", \"f1\", \"PR-AUC\"])\n",
    "\n",
    "                #Best hyperparam combinations of model for iterating train-valid \n",
    "                print(results_df.groupby([\"model\", \"p\", \"q\", \"r\"])[[\"f1\"]].mean().idxmax())\n",
    "                \n",
    "                #Plotting results (hyperparams vs mean f1)\n",
    "                for i,v in enumerate([\"p\", \"q\", \"r\"]):\n",
    "                    results_df.groupby([v])[\"f1\"].mean().plot(ax=axs[i])\n",
    "                plt.show()\n",
    "                    \n",
    "            else:\n",
    "                for p in hyperparams_small[modelname][\"contamination\"]:\n",
    "                    for q in hyperparams_small[modelname][\"n_neighbors\"]:        \n",
    "                        r=\"N/A\" \n",
    "                        #Instantiating model\n",
    "                        model=LocalOutlierFactor(contamination=p, n_neighbors=q, novelty=True)\n",
    "                        model.fit(X_train_small_1)\n",
    "                        y_scores=model.decision_function(X_train_small_2)\n",
    "                        y_pred=model.predict(X_train_small_2)\n",
    "\n",
    "                        #The algos predicts {1,-1} for {normal, anomalous} case, whereas original y has {0,1}\n",
    "                        y_pred=list(map(lambda x: 1 if(x==-1) else 0, y_pred))\n",
    "\n",
    "                        #Collating results\n",
    "                        results.append([i+1, j+1, modelname, p, q, r, recall_score(y_train_small_2,y_pred), f1_score(y_train_small_2,y_pred), average_precision_score(y_train_small_2,y_scores)])\n",
    "\n",
    "                #Dumping results into a df for pandas manipulation\n",
    "                results_df=pd.DataFrame(results, columns=[\"Train\", \"Valid\", \"model\", \"p\", \"q\", \"r\", \"recall\", \"f1\", \"PR-AUC\"])\n",
    "\n",
    "                #Best hyperparam combinations of model for iterating train-valid \n",
    "                print(results_df.groupby([\"model\", \"p\", \"q\", \"r\"])[[\"f1\"]].mean().idxmax())\n",
    "\n",
    "                #Plotting results (hyperparams vs mean f1)\n",
    "                for i,v in enumerate([\"p\", \"q\", \"r\"]):\n",
    "                    results_df.groupby([v])[\"f1\"].mean().plot(ax=axs[i])\n",
    "                plt.show()\n",
    "        \n",
    "        #Dumping all model results for iterating train-valid  into a df for pandas manipulation\n",
    "        results_df_1=pd.DataFrame(results, columns=[\"Train\", \"Valid\", \"model\", \"p\", \"q\", \"r\", \"recall\", \"f1\", \"PR-AUC\"])\n",
    "\n",
    "        #Best model and hyperparam combinations for iterating train-valid group\n",
    "        print(results_df_1.groupby([\"model\", \"p\", \"q\", \"r\"])[[\"f1\"]].mean().idxmax())\n",
    "                \n",
    "#Collating results in a df\n",
    "results_df_main=pd.DataFrame(results, columns=[\"Train\", \"Valid\", \"model\", \"p\", \"q\", \"r\", \"recall\", \"f1\", \"PR-AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
